---
title: "Project 4 - Recommender Systems"
author: "Marko Konte"
date: "4/7/2020"
output: html_document
---

### Step 0 Explanation of what the approach is

○ Pairing combination: 1+6
	§ 1 
		□ Algorithm: A1= Stochastic Gradient Descent
		□ Regularization: R1 + R2
			® R1= Penalty of Magnitudes
			® R2= Bias and Intercepts
		□ Postprocessing: P3= Kernel ridge regression 
	§ 6
		□ Algorithm: A2 = Gradient Descent with Probabilistic Assumptions
		□ Regularization: None
		□ Postprocessing: P2 = KNN
○ Goal: (A1+R1+R2) vs A2 given P2 (Stochastic Gradient Descent + Penalty of Magnitudes+ Bias and Intercepts) vs Gradient Descent with Probabilistic Assumptions given KNN

### Step 1 Load Data and Train-Test Split

```{r setup, include=FALSE}

library(dplyr)
library(tidyr)
library(ggplot2)
data <- read.csv('data/ratings.csv')
movies <- read.csv('data/movies.csv')
tags <- read.csv('data/tags.csv')

data <- left_join(data, tags, by = c("userId"))
colnames(data) <- c("userId", "movieId", "rating", "timestamp", "movieIdTag", "tag", "timestampTag")
data <- left_join(data, movies, by = c("movieId"))

set.seed(0)
test_idx <- sample(1:nrow(data), round(nrow(data)/5, 0))
train_idx <- setdiff(1:nrow(data), test_idx)
data_train <- data[train_idx,]
data_test <- data[test_idx,]


```

### Step 2 Matrix Factorization
#### Step 2.1 Algorithim and Regularization
○ Algorithim and Regularization Techniques
	§ 1 
		□ Algorithm: A1= Stochastic Gradient Descent
		    - This approach takes the very effective iterative way of using random inputs to build on past performance of a model in order to accurately make a model. The logic behind gradient descent is that it finds the local minimum in the differentiable function which denotes the least amount of error in the recommendation sample the algorithim is working on. The stochastic part of the equation denotes the randomization of inputs that are included in these types of recommender systems. It is necessary since the algorithim will not know the previous preferences of any new user requesting a recommendation. 
		□ Regularization: R1 + R2
			® R1= Penalty of Magnitudes
			    - This proces refers to the way of regularizing the combination of connections made by a recommender algorithim and generalizing them for better ways to predict future instances, which is important to prevent overfitting. Popular ways of doing this is using SVD (Singular value decomposition) and other general 'Probabilistic Matrix Factorization' techniques. In our approach, the best way to account for the magnitudes of inputs is by studying impact of the Lambda variable. 
			® R2= Bias and Intercepts
			    - Within a recommendation system like predicting movie tastes, there is the understood reality that some people will rate the same movie differently, even if they are rated similarly in their tastes. For example, some people may rate movies in general higher, or certain genres may get higher ratings than others in a systemic way. There are many elements of a rating that need to be taken into account: Global average, item bias, user bias, and user- item interaction. Averaging these items will allow each one users answers to be generalized enough to prevent similar level of overfitting. 
	§ 6
		□ Algorithm: A2 = Gradient Descent with Probabilistic Assumptions
		        - Similar to using penalty of magnitudes as a regularization technique, this internalizes the probabilities of gradient descent within the algorithim used in the recommender system. These algorithims take point estimates of each observation for the purpose of inferring the full posterior distribution over them. 
		□ Regularization: None

○ Goal: (A1+R1+R2) vs A2 given P2
	§ (Stochastic Gradient Descent + Penalty of Magnitudes+ Bias and Intercepts) vs Gradient Descent with Probabilistic Assumptions given KNN


```{r cars}
U <- length(unique(data$userId))
I <- length(unique(data$movieId))
source("lib/Matrix_Factorization.R")

```

### 2.2 Parameter Tuning
You can also embed plots, for example:

```{r pressure, echo=FALSE}

source("lib/cross_validation_proj4.R")
f_list <- seq(1, 2, 1)
l_list <- seq(-2, -1, 1)
f_l <- expand.grid(f_list, l_list)

# Default values: 
##f_list <- seq(10, 20, 10)
##l_list <- seq(-2, -1, 1)

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


```{r}

str(data)

result_summary <- array(NA, dim = c(nrow(f_l), 1, 16))
run_time <- system.time(for(i in 1:nrow(f_l)){
    par <- paste("f = ", f_l[i,1], ", lambda = ", 10^f_l[i,2])
    cat(par, "\n")
    current_result <- cv.function(data, K = 4, f = f_l[i,1], lambda = 10^f_l[i,2])
    result_summary[,,i] <- matrix(unlist(current_result), ncol = 10, byrow = T) 
    print(result_summary)
})

save(result_summary, file = "output/rmse.Rdata")

# Default values
## result_summary <- array(NA, dim = c(nrow(f_l), 10, 4)) 
##     current_result <- cv.function(data, K = 5, f = f_l[i,1], lambda = 10^f_l[i,2])


```


```{r}

load("output/rmse.Rdata")
rmse <- data.frame(rbind(t(result_summary[1,,]), t(result_summary[2,,])), train_test = rep(c("Train", "Test"), each = 4), par = rep(paste("f = ", f_l[,1], ", lambda = ", 10^f_l[,2]), times = 2)) %>% gather("epoch", "RMSE", -train_test, -par)
rmse$epoch <- as.numeric(gsub("X", "", rmse$epoch))
rmse %>% ggplot(aes(x = epoch, y = RMSE, col = train_test)) + geom_point() + facet_grid(~par)

```

#### 2.3 Evaluation on the model without Postprocessing

```{r}
result <- gradesc(f = 10, lambda = 0.1,lrate = 0.01, max.iter = 100, stopping.deriv = 0.01,
                   data = data, train = data_train, test = data_test)

save(result, file = "output/mat_fac.RData")

```


```{r}
load(file = "output/mat_fac.RData")
library(ggplot2)

RMSE <- data.frame(epochs = seq(10, 100, 10), Training_MSE = result$train_RMSE, Test_MSE = result$test_RMSE) %>% gather(key = train_or_test, value = RMSE, -epochs)

RMSE %>% ggplot(aes(x = epochs, y = RMSE,col = train_or_test)) + geom_point() + scale_x_discrete(limits = seq(10, 100, 10)) + xlim(c(0, 100))

```


## 3 Postprocessing

After matrix factorization, postporcessing will be performed to improve accuracy.
The referenced papers are:

- Postprocessing Approaches:
    -- P3= Kernel ridge regression
            - A very powerful approach to recommender systems is utilizing Singular Value Decomposition (SVD) methods and combing them with the Kernel Ridge Regression method. This method allows for minimizing of squared loss with a squared normm regularization term, it combines the parameters in a plane of kernels belonging to different methods. KRR does this by building a seperate linear model for each movie (which is why it is more computationally expensive) from using the regularization methods from SVD to come to a final prediction. 
    
	-- P2 = KNN: 
	        - As opposed to KRR which builds its own linear model and then regularized, the K Nearest Neighbor methood identifies the suggestions that are most similar after generating a plane based on inputted matrix factor from an SVD method. KNN uses the plane generated by the SVD process in order to identify the nearest instances for future recommendation results. This is also why this method is sighltly more computationally fast.  
	        
```{r}

pred_rating <- t(result$q) %*% result$p
#define a function to extract the corresponding predictedrating for the test set.
extract_pred_rating <- function(test_set, pred){
  pred_rating <- pred[as.character(test_set[2]), as.character(test_set[1])]
  return(pred_rating)
}
#extract predicted rating
pred_test_rating <- apply(data_test, 1, extract_pred_rating, pred_rating)


#mean(P)
pred_mean <- mean(pred_test_rating)
#mean(test)
mean_test_rating <- mean(data_test$rating)

#mean(test) - mean(P)
mean_diff <- mean_test_rating - pred_mean

data_test$pred <- pred_test_rating
data_test$pred_adj <- pred_test_rating + mean_diff

boxplot(data_test$pred_adj ~ data_test$rating)
#calculate RMSE
rmse_adj <- sqrt(mean((data_test$rating - data_test$pred_adj)^2))
cat("The RMSE of the adjusted model is", rmse_adj)


```


        