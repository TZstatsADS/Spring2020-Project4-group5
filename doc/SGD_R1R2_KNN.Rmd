---
title: "A1+R1+R2+P2"
author: "Ziqin Zhao, Xinlin Zhang"
date: "4/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Project 4 --- Part I (A1+R1+R2+P2)

Algorithm : Stochastic Gradient Descent (A1)

Regularization : Penalty of Magnitudes (R1) Bias and Intercepts (R2) 

Postprocessing : SVD with KNN (P2)

### Step 1 Load Data and Train-test Split
```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(coop)
data <- read.csv("../data/ml-latest-small/ratings.csv")


set.seed(0)
test_idx <- sample(1:nrow(data), round(nrow(data)/5, 0))
train_idx <- setdiff(1:nrow(data), test_idx)
data_train <- data[train_idx,]
data_test <- data[test_idx,]
```

###Step 2 Matrix Factorization
#### Step 2.1 Algorithm and Regularization
```{r}
U <- length(unique(data[1:5000,]$userId))
I <- length(unique(data[1:5000,]$movieId))
```

#### Step 2.2 Parameter Tuning

# Plot our tunning parameters for A1+R1+R2 (only 5000 rows)
```{r}
source("../lib/cross_validation_A1+R1+R2.R")  
source("../lib/Matrix_Factorization_A1+R1+R2.R")

f_list <- seq(10, 20, 10)
l_list <- seq(-2, -1, 1)
f_l <- expand.grid(f_list, l_list)

```

```{r eval=FALSE}
# use cross validation = 5 to get the rmse
result_summary_r12 <- array(NA, dim = c(nrow(f_l), 10, 4)) 
run_time <- system.time(for(i in 1:nrow(f_l)){
    par <- paste("f = ", f_l[i,1], ", lambda = ", 10^f_l[i,2])
    cat(par, "\n")
    current_result <- cv.function.r12(data[1:5000,], K = 5, f = f_l[i,1], lambda = 10^f_l[i,2])
    result_summary_r12[,,i] <- matrix(unlist(current_result), ncol = 10, byrow = T) 
    print(result_summary_r12)
})

save(result_summary_r12, file = "../output/rmseR12.Rdata")
```


```{r}
load(file = "../output/rmseR12.Rdata") 

rmse <- data.frame(rbind(t(result_summary_r12[1,,]), t(result_summary_r12[2,,])), train_test = rep(c("Train", "Test"), each = 4), par = rep(paste("f = ", f_l[,1], ", lambda = ", 10^f_l[,2]), times = 2)) %>% gather("epoch", "RMSE", -train_test, -par)
rmse$epoch <- as.numeric(gsub("X", "", rmse$epoch))
rmse %>% ggplot(aes(x = epoch, y = RMSE, col = train_test)) + geom_point() + facet_grid(~par)
```
From the plot, we use parameters f = 10 and lambda=0.1 as our basic model.

### Step 3 Postprocessing: SVD with KNN

```{r}
set.seed(0)

#get the subset of train set and test set from the first 5000 data
test_idx2 <- sample(1:5000, 5000/5, 0)
train_idx2 <- setdiff(1:5000, test_idx2)
data_train2 <- data[train_idx2,]
data_test2 <- data[test_idx2,]
```
```{r eval=FALSE}
resultr12 <- gradesc.r12(f = 10, lambda = 0.1,lrate = 0.01, max.iter = 100,
                         stopping.deriv = 0.01,data = data2, 
                         train = data_train2, test =data_test2)
save(resultr12, file = "../output/mat_facR12.RData")
```

```{r}
load(file = "../output/mat_facR12.RData")
result<-resultr12
train<-data_train2
```

```{r}
source("../lib/KNN.R")
KNN_Rating <- KNN.post(data_train2, result) # Rating after applying KNN
Rating <- t(result$p)%*%(result$q)  # Rating without knn 
#Knn_Rating: (32*2427: the number in the matrix is the score of ratings) 
```

```{r}
RMSE <- function(rating, est_rating){
  sqr_err <- function(obs){
    sqr_error <- (obs[3] - est_rating[as.character(obs[1]), as.character(obs[2])])^2
    return(sqr_error)
  }
  return(sqrt(mean(apply(rating, 1, sqr_err))))  
}

tibble(rating=c("Without KNN","With KNN"),
       train=c(RMSE(data_train2,Rating),RMSE(data_train2,KNN_Rating)),     
       test=c(RMSE(data_test2,Rating),RMSE(data_test2,KNN_Rating)))

# train mse with knn is the same as mse without knn. 

```



